{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7512e75-f885-4cc2-8a9a-ef96600c521d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "import deepl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd3b9df-8f45-4759-b814-cf1d546833d6",
   "metadata": {},
   "source": [
    "## Load Model Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a7a451f-fdf1-469b-b66d-8099af63b601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-01 14:21:58.843188: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2024-02-01 14:21:58.843222: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-02-01 14:21:58.843230: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-02-01 14:21:58.843284: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-02-01 14:21:58.843309: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('FairyTaleProject.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22ed2a0d-4989-4f17-bdc0-358970131f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pkl','rb') as file:\n",
    "    tokenizer = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2df1e3be-8562-4e99-b396-aee19d50530d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training_history.pkl','rb') as file:\n",
    "    history = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d8a0360-dc4f-47ef-b2b7-2d5e014907a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('deepl_api.txt','r') as file:\n",
    "    deepl_api = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c053891-e5a0-4bbf-bf5d-25ec6f92d3a4",
   "metadata": {},
   "source": [
    "## Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a5d56ed-22c1-4aa5-83b7-a854a6a61074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepl_translator(text):\n",
    "    auth_key = deepl_api  # Replace with your key\n",
    "    translator = deepl.Translator(auth_key)\n",
    "    \n",
    "    result = translator.translate_text(text, target_lang=\"TR\")\n",
    "    return result.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd49619a-9218-45b6-939f-233e8b49845b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yaseminsungur/anaconda3/envs/btk2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "Thanks for being a Gradio user! If you have questions or feedback, please join our Discord server and chat with us: https://discord.gg/feTf9x3ZSB\n",
      "Running on public URL: https://e39c5561c47b827bf1.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://e39c5561c47b827bf1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 301ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-01 14:22:32.574132: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "max_sequence_len = 403\n",
    "\n",
    "def generator(markdown,seed_text,next_words):\n",
    "  for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "      if index == predicted:\n",
    "        output_word = word\n",
    "        break\n",
    "    seed_text += \" \" + output_word\n",
    "\n",
    "  translated_text = deepl_translator(seed_text)  \n",
    "    \n",
    "  return seed_text,translated_text\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=generator,\n",
    "    \n",
    "    inputs=[\n",
    "        gr.Markdown('# FairyMairy'),\n",
    "        gr.Textbox(lines=15),\n",
    "        gr.Slider(maximum=30,step=1)\n",
    "        \n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(lines=10,label='İngilizce'),\n",
    "        gr.Textbox(lines=10,label='Türkçe')\n",
    "    ],\n",
    ")\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe768486-7146-4bdc-bac9-292854352674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "#demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae70523-45c8-4367-b059-e31477eedc48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:btk2]",
   "language": "python",
   "name": "conda-env-btk2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
